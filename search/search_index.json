{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the RootPainter Documentation","text":"<p>RootPainter is a powerful image annotation tool designed for semantic segmentation tasks, enabling efficient collaboration between a client GUI (Painter) and a server (Trainer).</p> <p>This site provides all the information you need to get started with RootPainter, whether you are using it for your own projects or contributing to its development.</p>"},{"location":"#for-users","title":"For Users","text":"<p>New to RootPainter? Start here to learn how to install and use the application for your image annotation needs.</p> <ul> <li>User Guide: A comprehensive guide to using RootPainter.</li> <li>Frequently Asked Questions (FAQ): Find answers to common questions.</li> </ul>"},{"location":"#for-developers","title":"For Developers","text":"<p>Want to contribute to RootPainter or learn more about its internals? The developer documentation provides a deep dive into the architecture and development practices.</p> <ul> <li>Painter (Client): Information about the painter GUI.</li> <li>Testing: How to run tests for the trainer.</li> <li>Icons: Notes on application icons.</li> </ul> <p>We are constantly working to improve RootPainter and its documentation. If you have feedback or questions, please don't hesitate to reach out.</p>"},{"location":"faq/","title":"Faq","text":""},{"location":"faq/#frequently-asked-questions","title":"Frequently asked questions","text":""},{"location":"faq/#question-how-do-i-skip-to-images","title":"Question - How do I skip to images?","text":"<p>If you want to skip back to the first few images it is possible to do this with the back/previous button but for large projects this can take a while as you will need to wait for each image to load. A more efficient method is possible using the metrics plot.</p> <p>With the project open, go to the extras menu and click on view metrics plot. Then click on the image point in the metrics plot and it will take you to the corresponding image in the viewer.</p>"},{"location":"faq/#question-should-i-let-training-finish","title":"Question -  Should I let training finish?","text":"<p>If you stop annotating and let  training continue it will eventually reach 60 epochs out of 60 with no progress. It may not be easy for you to do this with your hardware (the free version of colab, for example, has time contraints). You may be wondering if leaving training to finish is essential and if it makes the model more robust.</p> <p>We experimented with this in the original study. See Figure 8. In short, we found that letting the model train to completion can provide some marginal benefits in some cases.</p> <p>I suspect this is hardware specific. If you have slow hardware (such as google colab) then it's more likely that the hardware is a bottleneck and it is training time (rather than amount of annotation) that is the main bottleneck preventing performance improvements. In this case letting the model train for a bit longer may provide more benefits.</p> <p>RootPainter provides an interactive-machine-learning experience where what you see is what you get. Meaning the segmentations you see in the interface are an accurate representation of the models accuracy on the data you wish to process. It is OK to stop training and the model for segmentation at any time, ideally once you are happy with what you see in terms of segmentation quality. The decisions of when to stop annotation/interactive-training may also be informed by the corrective-metrics plot which is available from the extras menu. </p>"},{"location":"faq/#question-how-do-i-decide-when-the-model-is-good-enough","title":"Question - How do I decide when the model is good enough?","text":"<p>My personal recommendation is to use the metrics plot that is available from the extras menu. When you annotate images in the RootPainter interface, if you annotate all the error (or approximately all the error) in each image then you have a measure of the model performance/generalisation to new data.</p> <p>The metrics plot is created by computing the difference between the initial segmentation provided by RootPainter and the segmentation with the corrections assigned. This difference gives us a measure of error for each image annotated correctively.</p> <p>The metrics plot allows you to compute the error in terms of accuracy, dice, precision, recall, area etc. </p> <p>In the RootPainter paper we have examples of some corrective-metrics plots for each of the datasets investigated </p> <p>You can see from the first two rows in the linked figure (labelled a) that the dice goes above 0.8 for the roots datasets. If you are also training a model for root segmentation and the running mean of your corrective dice as computed using RootPainter's integrated metrics plot is above 0.8 then you could report this (and perhaps include the plot itself in the supplementary material) as a measurement that shows your model is accurate. Exporting the plot as an image is possible by right clicking on it.</p> <p>The plot relies on you correcting all the error in each image. If you skip a large part of the error in each image (don't annotate it correctively) then I would not say that the corrective-metrics are an accurate measure of error.</p> <p>The other indicator is qualitative. You should in general have some idea that the model is doing the job from looking at the segmentations in the interface. Reporting these qualitative results in your paper is best done using the 'extract composites' feature available from the extras menu. These composites are image files that show the segmentation in combination with the input image and can be used as figures in presentations, your paper or supplement.</p> <p>Knowing exactly how accurate a model needs to be for your specific research question is currently out of the scope of the RootPainter software. This depends on the size of the effect you are measuring, the number of samples and the variance present in your dataset. It may be that a dice or accuracy of just 0.4 (for example) may be enough for your research question and object of interest. The metrics provided in the corrective-metrics plot may be expanded on upon request. The area metric, which allow you to see if a model is over- or underestimating your structure of interest based on your corrections was added recently.</p>"},{"location":"faq/#question-why-is-the-segmentation-not-loading","title":"Question - Why is the segmentation not loading?","text":""},{"location":"faq/#answer","title":"Answer:","text":"<p>My best guess is that there is a delay in sync or the sync software is not set up and/or working properly. There might be another problem (such as a bug with RootPainter) but here are a few steps to help you isolate the problem:</p> <ol> <li> <p>If using google drive + colab, I suggest changing google drive (local client) to mirror instead of stream. It might take a few hours to download your files (potentially) but then this may speed up/fix sync.</p> </li> <li> <p>Check the sync directory is speci\ufb01ed correctly. In your home directory i.e the user directory, there is a \ufb01le called root_painter_settings.json. You can open this \ufb01le in a text editor to see which directory is speci\ufb01ed as the sync directory. You can also just manually set a new sync directory (to make sure it is correct) using the option from the extras menu. The sync directory should be set as a path to the drive_rp_sync folder if inside your google drive on your local computer, if you are following the colab tutorial. Otherwise it should be the folder setup to share data between the RootPainter client and server (which was also specified when you started the RootPainter server).</p> </li> <li> <p>Check segmentation instructions are being created.  Without starting the trainer (server) , open the client (GUI) and go to an image that doesn\u2019t already have a segmentation. You should see \u2018loading segmentation\u2019 in the client. At this point the client should also create an instruction (just a text \ufb01le) in the instructions folder which is inside the sync folder. If you are following the colab tutorial then your sync folder is a folder in your google drive called drive_rp_sync. This instruction \ufb01le is a text \ufb01le that tells the server which image to segment. You can con\ufb01rm this \ufb01le is being created correctly by checking the instruction folder on your computer and checking if the instruction \ufb01le exists. You need to do this step without the server running, because if the server is running it will delete the instruction as soon as it has read it. I suggest checking your local google drive folder \ufb01rst. This should exist on your local machine. If the instructions are never being created in the correct location then there may be a problem with the sync directory or perhaps some permissions issues.</p> </li> <li> <p>(if you are using colab) Check segmentation instruction is being synced to google drive. The colab tutorial assumes you have set up and installed google drive for desktop. If you have google drive for desktop up and running and then the sync folder speci\ufb01ed correctly then instructions should get created in your local instructions folder. These instructions should also get synced to google drive. If you have \ufb01rst con\ufb01rmed the instructions are created correctly then you can check your online google drive to see if they are synced with google's servers. Go to the following url: https://drive.google.com/drive/u/0/my-drive (or just navigate to your google drive online using a web browser) and then go to the drive_rp_sync folder (that was created as part of the colab tutorial). Inside you should see an instructions folder and in the instructions folder should be the instruction \ufb01le that was created locally on your computer using the RootPainter client (the GUI application). If you do see the instruction \ufb01le locally but you don\u2019t see it online then it indicates there is a problem with synchronization of \ufb01les from your computer to google drive.</p> </li> <li> <p>(if you are using colab). Inspect the status of google drive for desktop (that should be running on your computer). It could be delayed, busy syncing many \ufb01les or not set up to sync the correct locations. In some rare cases it can be buggy and you may want to try uninstalling it and reinstalling again to get sync working properly.</p> </li> <li> <p>Check for errors in the server output. When running both client and server together, inspect the console output for the server in colab. You should see output after you start the server (which should be left running whilst using RootPainter). For example, if you view an image in the client named CHNCXR_0068_0.jpg that does not have a segmentation yet, it then you should see the following output from the server console:</p> </li> </ol> <p>execute_instruction segment</p> <p>ensemble segment CHNCXR_0068_0.jpg, dur 0.2</p> <p>Seconds to segment 1 images:  0.237</p> <p>Note: Sometimes due to slow sync time it will appear after a delay, so wait a couple of minutes just to be sure there isn\u2019t some delay in sync.</p> <ol> <li>Inspect the console output for the client. The most recent versions of RootPainter (https://github.com/Abe404/root_painter/releases/) should open a black console that displays error messages from time to time. Perhaps something is going wrong and the output will give us a clue.</li> </ol> <p>Hopefully, even if it doesn\u2019t solve the problem, these instructions will help you get more information that will help us figure out what is going wrong. If you think you have found a bug in the software then feel free to report an issue (https://github.com/Abe404/root_painter/issues) </p>"},{"location":"faq/#question-on-ubuntu-i-get-an-error-related-to-xcb","title":"Question - On ubuntu I get an error related to xcb","text":"<p>The error message may be similar to the following: <pre><code>qt.qpa.plugin: Could not load the Qt platform plugin \"xcb\" in \"\" even though it was found.\nThis application failed to start because no Qt platform plugin could be initialized. Reinstalling the application may fix this problem.\n\nAvailable platform plugins are: eglfs, linuxfb, minimal, minimalegl, offscreen, vnc, wayland-egl, wayland, wayland-xcomposite-egl, wayland-xcomposite-glx, webgl, xcb.\n</code></pre></p> <p>As outlined in this forum discussion answer installing libxcb-xinerama0 appears to fix the problem. Which can be done with the following command:</p> <pre><code>sudo apt-get install libxcb-xinerama0\n</code></pre> <p>If that doesn't work another solution I have found at this post on github has also worked. <pre><code>sudo apt-get install '^libxcb.*-dev' libx11-xcb-dev libglu1-mesa-dev libxrender-dev libxi-dev libxkbcommon-dev libxkbcommon-x11-dev\n</code></pre></p>"},{"location":"faq/#question-how-can-i-use-rootpainter-for-a-multiclass-segmentation-task","title":"Question - How can I use RootPainter for a multiclass segmentation task?","text":"<p>It's possible to train a binary single class model for each of your classes. A more experimental (developer friendly) multiclass version of RootPainter is also availale in the branch named 'multiclass'. When more testing has been done, I will make it available in a more user-friendly client installer.</p> <p>A colab notebook is available that runs the multiclass version of RootPainter. Classes can be specified when creating a project. Each class that is specified implicity has it's own background, thus a backround class does not need to be explicitly specified. Foreground and background annotation should be assigned correctively for each class for each image.</p> <p>The multiclass client can be ran from source by using git to clone the repo and swith to the multiclass branch. <pre><code>git clone --single-branch --branch multiclass https://github.com/Abe404/root_painter.git\n</code></pre></p>"},{"location":"faq/#question-i-already-have-a-trained-model-do-i-still-need-a-gpu-for-segmentation","title":"Question - I already have a trained model. Do I still need a GPU for segmentation?","text":"<p>Yes, a GPU is required for both training and segmentation. Other functionality, such as generating composites, converting segmentations for Rhizivision explorer and extracting measurements does not require a powerful GPU and can be computed using the client only.</p>"},{"location":"quickstart/","title":"Quickstart","text":""},{"location":"quickstart/#a-concise-guide-to-using-rootpainter","title":"A concise guide to using RootPainter","text":"<p>This is a short guide to help users remember the key steps to obtain measurements using RootPainter.  See main page for additional documentation including links to videos, papers and a colab notebook.</p> <p>Note: RootPainter is primarily used for root segmentation but works for many other objects. Modify instructions accordingly.</p> <ol> <li>Open RootPainter and select 'Create training dataset'. Create a training dataset using 1000 images with a target size of 750 pixels and 1 tile per image.</li> <li>Create a new project referencing the new training dataset as the source dataset.</li> <li>Annotate 2 images that contain both roots and soil. Don\u2019t annotate more than (roughly estimated) 10x more soil than roots. Don\u2019t annotate images that don\u2019t contain roots.</li> <li>After 2 images are annotated click start training (from network menu) to start the network training on your annotations and annotated images.</li> <li>Annotate 4 more images. At this point, you are not interested in the model segmentation. The point is just to provide some clear annotations to get training started. <ul> <li>Note: if using colab or a slow server, I suggest to annotate 12-18 initial clear examples instead of 6. (A general rule is that you should not switch to corrective annotation until your model is approximately predicting the object of interest. There is not much point in correcting random noise).</li> </ul> </li> <li>When 6 images are annotated (clear examples, including both root and soil), then on image 7 switch to corrective annotation. Inspect the model prediction (segmentation) for the image and assign foreground and background annotation to all of the errors in each image. Corrections don\u2019t need to be 100% perfect in terms of capturing absolutely every incorrect pixel but aim to avoid annotating foreground on the soil and background on the root as these annotation errors are particularly problematic.</li> <li>Continue to annotate correctively, annotating the errors in each image. If root is missing from the segmentation, annotate the root or missing region of the root in red. If soil/background is predicted as root then annotate this region in green. Use a big brush to annotate the background because it is quicker. Use a small brush to annotate the roots to avoid annotating roots on the soil.</li> <li>If you are using colab or another slow server or have a slow connection to your GPU server, you may wish to segment some images in advance of viewing them to speed up the annotation process. To enable this select 'pre-segment' from the options menu and input a value greater than 0. I suggest starting 1 and gradually increasing as you start to annotate faster, eventually you may wish to increase this to a value between 5-10 to avoid spending too much time waiting for segmentations to correct.</li> <li>After a while (30 images) check the metrics plot. This is available from the extras menu. Show the metrics plot to track progress. Metrics of interest include:<ul> <li>Dice - harmonized mean of precision and recall.</li> <li>Recall - the number of root pixels predicted to be root as ratio of total root pixels in the image.</li> <li>Precision - number of predicted pixels that were actually root out of total predicted root pixels.</li> <li>Accuracy - % of image predicted correctly</li> <li>False positives - number of soil pixels predicted as root.</li> <li>False negatives - number of root pixels predicted as soil.</li> <li>Area including predicted-correct - This measure of area error gives an indication if area is being over or under estimated by the model. See \u2018corrected area\u2019 to get an idea of absolute values of area. You can then observe rolling averages and see how big area error is in relation to the correct area (i.e does area error matter for your downstream analysis and overarching/primary research question).</li> </ul> </li> <li>You can plot the metrics over time and compute rolling average within RootPainter. If metrics get very high and stay high i.e plateau, then you may gain confidence that further annotation is not required. You can click on points in the metrics plot to inspect outliers that may be related to artifacts in the data or perhaps errors in annotation. The metrics plot is only a reliable indicator of model error if you annotated approximately all the error in each image as the metrics are computed by taking the difference between the predictions and corrected segmentation (segmentation with corrections assigned).</li> <li>As you progress through the images, you should be able to annotate faster and faster as the images should require less annotation as the model improves. This means you can likely skip many of the easier images and begin to target your corrective annotation towards anomalies or outliers.</li> <li>Once happy with the model you can segment the full folder of the original images by going to the network menu and clicking \u2018segment folder\u2019. Put the output segmentations in the results folder of the project i.e. project_name/results/model_20_seg. You can name these output folder whatever you like. I like to use the model name so I know which model was used to generate the output segmentations.</li> <li>Extract measurements using the extract length option from the measurements menu in RootPainter to get a CSV you can analyze in other software.</li> <li>Get diameter and diameter classes or other traits using RhizoVisionExplorer (RVE) (downloaded separately). To prepare RootPainter output segmentation for analysis in RVE, use the convert segmentations for RVE option available from the RootPainter extras menu.</li> <li>To inspect final segmentation output (not the segmentation generated during the interactive training process but the segmentation generated from the original images using the segment folder option) use the \u2018extract composite\u2019 functionality. This is useful for generating figures to use in presentations and in manuscripts. </li> <li>To communicate that your model was accurate you can use both composites of random images and the metrics plot. The metrics plot can be right-clicked giving an export option. You may export to various formats including a PNG that can be inserted into manuscript (supplementary material etc).</li> <li>If you generate the composites or extract measurements and then decide later that you want to further train the model to get even better or more accurate results then you should reopen the project and annotate more images (correcting the errors on new images) whilst training. Then you should repeat steps 7-15. But give the output segmentation and results files new names, assuming a new more accurate model has been generated.</li> <li>To include more images in your training dataset, then you must add these images to the existing dataset folder and then run 'extend dataset' from the extras menu with the project open.</li> <li>Note: RootPainter displays a message stating X epochs out of 60 (or similar) when it is training. You don\u2019t have to wait until 60 epochs without progress to use the model trained, this is optional and may only be marginally beneficial.</li> </ol>"},{"location":"developer/painter/","title":"Painter (Client) Developer Documentation","text":"<p>This document provides instructions for developers working on the Painter (client) component of RootPainter. For user-facing documentation, please refer to the main project <code>README.md</code> and the <code>docs</code> directory.</p> <p>Installers for OSX, Windows and Debian based linux distributions are available from the releases page, which is recommended for most users. The instructions below are for building the painter from source.</p> <p>The server (trainer) must be running for the client to function.</p>"},{"location":"developer/painter/#install-dependencies","title":"Install dependencies","text":"<p>It is recommended to use a virtual environment. The following commands should be run from within the <code>painter</code> directory.</p> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"developer/painter/#windows-build-dependencies","title":"Windows Build Dependencies","text":"<p>For generating an executable on Windows, ensure that the following are installed and available in your system's PATH: - NSIS tools - C++ Redistributable for Visual Studio 2012 - Windows 10 SDK</p>"},{"location":"developer/painter/#running-from-source","title":"Running from source","text":"<pre><code>python root_painter/main.py\n</code></pre>"},{"location":"developer/painter/#building-the-application","title":"Building the application","text":"<p>To build the application for your current platform, run:</p> <pre><code>python scripts/run_pyinstaller.py\n</code></pre> <p>The output will be in the <code>dist/</code> directory.</p>"},{"location":"developer/painter/#building-installers","title":"Building Installers","text":"<p>Installers must be created on the target platform (e.g., a Windows installer must be built on Windows).</p>"},{"location":"developer/painter/#windows-exe","title":"Windows (.exe)","text":"<pre><code>makensis scripts/assets/Installer.nsi\n</code></pre>"},{"location":"developer/painter/#macos-pkg","title":"macOS (.pkg)","text":"<pre><code>pkgbuild --component dist/RootPainter.app --install-location /Applications dist/RootPainter.pkg\n</code></pre>"},{"location":"developer/painter/#debianubuntu-deb","title":"Debian/Ubuntu (.deb)","text":"<pre><code>bash scripts/make_deb_file\n</code></pre> <p>The output installer will be located at <code>dist/RootPainter.deb</code>. To install it:</p> <pre><code>sudo dpkg -i dist/RootPainter.deb\n</code></pre>"},{"location":"developer/testing/","title":"Trainer Tests","text":"<p>Tests for the server (trainer) component of RootPainter.</p> <p>To run the tests (from the <code>trainer</code> directory):</p> <pre><code>pytest tests/\n</code></pre>"},{"location":"developer/testing/#todo-roadmap","title":"TODO / Roadmap","text":"<ul> <li>Get tests to run without error.<ul> <li>figure out why tests are failing.</li> </ul> </li> </ul>"},{"location":"developer/trainer/","title":"Trainer (Server) PyTorch Installation","text":"<p>PyTorch is required but installing it is not always straightforward to automate from a script.</p> <p>Please see official instructions available here: https://pytorch.org/</p> <p>If the 'Stable' option does not work for you then it is recommended to try the Preview (Nightly) build.</p>"},{"location":"setup/diku_cluster/","title":"Diku cluster","text":""},{"location":"setup/diku_cluster/#diku-rootpainter-setup","title":"DIKU RootPainter Setup","text":"<p>This is is a guide for getting started with the RootPainter software using the diku servers and has been tested using python 3.7. RootPainter is described in this paper.</p>"},{"location":"setup/diku_cluster/#1-you-will-need-to-be-able-to-ssh-into-slurm-if-you-dont-have-one-already-then-add-an-entry-for-slurm-in-your-ssh-config-file-add-the-following-to-sshconfig-and-replace-kuid-with-your-own-ku-id","title":"1. You will need to be able to ssh into slurm. If you don't have one already then add an entry for slurm in your ssh config file. Add the following to ~/.ssh/config and replace KUID with your own KU ID.","text":""},{"location":"setup/diku_cluster/#host-slurm-hostname-a00552-user-kuid-proxycommand-ssh-q-w-hp-kuidssh-diku-imagesciencekudk","title":"<pre><code>Host slurm\n  Hostname a00552\n  User KUID\n  ProxyCommand ssh -q -W %h:%p KUID@ssh-diku-image.science.ku.dk\n</code></pre>","text":"<p>There is more information about working with slurm in the Slurm Wiki.</p> <ol> <li> <p>SSH into the server to set up the server component of RootPainter. <pre><code>ssh slurm\n</code></pre></p> </li> <li> <p>Clone the RootPainter code from the repository and then cd into the trainer directory (the server component). <pre><code>git clone --branch 0.2.4 https://github.com/Abe404/root_painter.git\ncd root_painter/trainer\n</code></pre></p> </li> <li> <p>To avoid alterating global packages. I suggest using a virtual environment. Create a virtual environment and activate it. <pre><code>python -m venv env\nsource ./env/bin/activate\n</code></pre></p> </li> <li> <p>Install dependencies in the virtual environment. (takes over 3 minutes) <pre><code>pip install torch==1.3.1 -f https://download.pytorch.org/whl/torch_stable.html\npip install -r requirements.txt\n</code></pre></p> </li> <li> <p>Run root painter to create the sync directory. <pre><code>python main.py\n</code></pre> You will be prompted to input a location for the sync directory. This is the folder where files are shared between the client and server. I will use ~/root_painter_sync. RootPainter will then create some folders inside ~/root_painter_sync</p> </li> <li> <p>Create a slurm job. Create a file named job.sh and insert the following. Modify the details based on your preferences.</p> </li> </ol>"},{"location":"setup/diku_cluster/#binbash-normal-cpu-stuff-allocate-cpus-memory-sbatch-ntasks1-cpus-per-task12-mem20000m-we-run-on-the-gpu-partition-and-we-allocate-1-titanrtx-gpu-sbatch-p-gpu-gresgputitanrtx1-we-expect-that-our-program-should-not-run-langer-than-3-hours-note-that-a-program-will-be-killed-once-it-exceeds-this-time-sbatch-time30000-your-script-in-this-case-write-the-hostname-and-the-ids-of-the-chosen-gpus-hostname-echo-cuda_visible_devices-python-mainpy","title":"<pre><code>#!/bin/bash\n# normal cpu stuff: allocate cpus, memory\n#SBATCH --ntasks=1 --cpus-per-task=12 --mem=20000M\n# we run on the gpu partition and we allocate 1 titanrtx gpu\n#SBATCH -p gpu --gres=gpu:titanrtx:1\n#We expect that our program should not run langer than 3 hours\n#Note that a program will be killed once it exceeds this time!\n#SBATCH --time=3:00:00\n\n#your script, in this case: write the hostname and the ids of the chosen gpus.\nhostname\necho $CUDA_VISIBLE_DEVICES\npython main.py\n</code></pre>","text":"<ol> <li> <p>Run the slurm job. <pre><code>sbatch job.sh\n</code></pre></p> </li> <li> <p>To mount the sync directory from your local machine you will need to install sshfs locally (SSH Filesystem client).</p> </li> </ol> <p>Debian / Ubuntu: <pre><code>sudo apt-get install sshfs\n</code></pre> OSX: <pre><code>brew cask install osxfuse\n</code></pre></p> <p>Windows: See sshfs-win</p> <ol> <li>Create the directory and mount the drive locally using sshfs. Replace KUID with your own KU ID. <pre><code>mkdir ~/Desktop/root_painter_sync\nsshfs -o allow_other,default_permissions KUID@slurm:/home/KUID/root_painter_sync ~/Desktop/root_painter_sync\n</code></pre> You should now be able to see the folders created by RootPainter (datasets, instructions and projects) inside ~/Desktop/root_painter_sync on your local machine  See lung tutorial for an example of how to use RootPainter to train a model.</li> </ol>"},{"location":"setup/local_server/","title":"Local Server Setup","text":"<p>These instructions are for setting up a local RootPainter server. If you do not have a suitable NVIDIA GPU with at least 8GB of GPU memory, the current recommendation is to use the Google Colab tutorial.</p> <p>For other remote server setups, see the sshfs server setup tutorial. You can also use Dropbox instead of sshfs.</p> <p>For the next steps I assume you have a suitable GPU and CUDA installed.</p> <ol> <li> <p>To install the RootPainter trainer:</p> <pre><code>pip install root-painter-trainer\n</code></pre> </li> <li> <p>To run the trainer. This will first create the sync directory.</p> <pre><code>start-trainer\n</code></pre> <p>Note: if you are installing the RootPainter trainer (server) from scratch on windows 11 I suggest these linked instructions.</p> <p>You will be prompted to input a location for the sync directory. This is the folder where files are shared between the client and server. For example, you can use <code>~/root_painter_sync</code>. RootPainter will then create some folders inside your sync directory. The server should print the automatically selected batch size, which should be greater than 0. It will then start watching for instructions from the client.</p> <p>You should now be able to see the folders created by RootPainter (datasets, instructions and projects) inside the sync directory on your local machine.</p> <p>For an example of how to use RootPainter to train a model, see the lung tutorial. It is now recommended to follow the Colab tutorial instructions but using your local setup instead of the colab server, as these are easier to follow than the lung tutorial.</p> </li> </ol>"},{"location":"setup/remote_server_sshfs/","title":"Remote server sshfs","text":""},{"location":"setup/remote_server_sshfs/#rootpainter-server-setup-with-sshfs","title":"RootPainter Server Setup with sshfs","text":"<p>This is is a guide for getting started with the RootPainter software using sshfs to connect the client and server components. RootPainter is described in this paper.</p> <p>This page includes instructions for setting up the server. After completing these instructions, you can follow the lung tutorial for an example of how to use RootPainter to train a model using the graphical client.</p> <p>I assume you have ssh access to a linux server with a suitable GPU and CUDA installed.</p> <ol> <li> <p>SSH into your server to set up the server component of RootPainter. <pre><code>ssh username@xxx.xxx.xxx.xxx\n</code></pre></p> </li> <li> <p>Clone the RootPainter code from the repository and then cd into the trainer directory (the server component). <pre><code>git clone https://github.com/Abe404/root_painter.git\ncd root_painter/trainer\n</code></pre></p> </li> <li> <p>To avoid alterating global packages. I suggest using a virtual environment. Create a virtual environment and activate it. This tutorial has been tested using python 3.7 <pre><code>python -m venv env\nsource ./env/bin/activate\n</code></pre></p> </li> <li> <p>Install dependencies in the virtual environment. (takes over 3 minutes) <pre><code>pip install torch\npip install -r requirements.txt\n</code></pre></p> </li> <li> <p>Run root painter. This will first create the sync directory. <pre><code>python main.py\n</code></pre> You will be prompted to input a location for the sync directory. This is the folder where files are shared between the client and server. I will use ~/root_painter_sync. RootPainter will then create some folders inside ~/root_painter_sync. The server should print the automatically selected batch size, which should be greater than 0. It will then start watching for instructions from the client.</p> </li> </ol> <p>If you see a batch size above 0 and 'Started main loop. Checking for instructions..' in the console output then this means you have been successful in starting the server and can move onto the next step.</p> <ol> <li>To mount the sync directory from your local machine you will need to install sshfs locally (SSH Filesystem client).</li> </ol> <p>If you are using Debian / Ubuntu <pre><code>sudo apt-get install sshfs\n</code></pre></p> <p>If you are using mac / OSX:</p> <p>Download the macFUSE dmg. Open the OSXFuse dmg and run the installer. Similarly, download the SSHFS pkg and run the downloaded SSHFS installer.</p> <p>If you are using Windows: See sshfs-win</p> <ol> <li>Create the directory that you will mount.</li> </ol> <p>You will need to run this command on your local machine.</p> <pre><code>mkdir ~/Desktop/root_painter_sync\n</code></pre> <ol> <li>mount the drive locally using sshfs. </li> </ol> <p>You will need to run the following command on your local machine. You should replace username with the username of the corresponding user on the server (the one you use when sshing into your server). You should also replace xxx.xxx.xxx.xxx with the ip address of your server. <pre><code>sudo sshfs -o allow_other,default_permissions username@xxx.xxx.xxx.xxx:/home/username/root_painter_sync ~/Desktop/root_painter_sync\n</code></pre></p> <p>You should now be able to see the folders created by RootPainter (datasets, instructions and projects) inside ~/Desktop/root_painter_sync on your local machine  See lung tutorial for an example of how to use RootPainter to train a model.</p>"},{"location":"setup/windows_trainer/","title":"Installing the RootPainter trainer (server) on windows 11.","text":"<p>Note: This tutorial assume you have a suitable GPU and CUDA already setup.</p> <ol> <li>Open powershell as administrator</li> <li>Set-ExecutionPolicy -ExecutionPolicy RemoteSigned</li> <li>Invoke-WebRequest -UseBasicParsing -Uri \"https://raw.githubusercontent.com/pyenv-win/pyenv-win/master/pyenv-win/install-pyenv-win.ps1\" -OutFile \"./install-pyenv-win.ps1\"; &amp;\"./install-pyenv-win.ps1\"</li> <li>Close and reopen powershell as administrator.</li> <li>pyenv install 3.10.11</li> <li>pyenv global 3.10.11</li> <li>pyenv version </li> <li>python -c \"import sys; print(sys.executable)\"</li> <li>pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 --index-url https://download.pytorch.org/whl/cu117</li> <li>pip install root-painter-trainer</li> <li>start-trainer</li> <li>You will be prompted to input the RootPainter Sync directory. You can just type root_painter_sync and press enter.</li> <li>The trainer should now be running and it should tell you your batch size (should be above 0) and if the GPU is available or not (It should say True).</li> </ol>"},{"location":"tutorials/cxr_lung/","title":"Cxr lung","text":""},{"location":"tutorials/cxr_lung/#rootpainter-chest-x-ray-segmentation-tutorial","title":"RootPainter Chest X-ray Segmentation Tutorial","text":"<p>This is a guide for using RootPainter to train a model to segment lungs in chest X-ray images.</p> <p>In this guide I assume you have the server component of RootPainter set up and running. I also assume the sync directory location to be ~/Desktop/root_painter_sync. Please modify accordingly.</p> <ol> <li> <p>Download and extract the Shenzhen Hospital X-ray Set: <pre><code>  wget http://openi.nlm.nih.gov/imgs/collections/ChinaSet_AllFiles.zip\n  unzip ChinaSet_AllFiles.zip\n</code></pre></p> </li> <li> <p>This step is optional but highly recommended. The images are big and more than we need for this tutorial. I suggest resizing the data to speed up the training process and data transfer. We will also use only 300 of the images.</p> </li> </ol> <p>Add the following to a file named resize_cxr.py: <pre><code>import os\nimport sys\nfrom multiprocessing import Pool\nimport random\n\nimport tqdm\nfrom skimage.io import imread, imsave\nfrom skimage import img_as_ubyte\nfrom skimage.transform import resize\n\nsrc_dir = sys.argv[1]\nout_dir = sys.argv[2]\nassert os.path.isdir(src_dir)\nassert not os.path.isdir(out_dir) and not os.path.isfile(out_dir)\nfnames = [a for a in os.listdir(src_dir) if os.path.splitext(a)[1] == '.png']\n\nif not os.path.isdir(out_dir):\n    os.makedirs(out_dir)\n\ndef resize_file(f):\n    out_path = os.path.join(out_dir, os.path.splitext(f)[0] + '.jpg')\n    if not os.path.isfile(out_path):\n        im = imread(os.path.join(src_dir, f))\n        h, w = im.shape[:2]\n        # resize so smallest dimension is 600\n        if w &lt; h:\n            new_w = 600\n            ratio = new_w/w\n            new_h = round(h * ratio)\n        else:\n            new_h = 600\n            ratio = new_h/h\n            new_w = round(w * ratio)\n        im = resize(im, (new_h, new_w))\n        imsave(out_path, img_as_ubyte(im))\n\nif __name__ == '__main__':\n    fnames = random.sample(fnames, k=300)\n    with Pool(processes=16) as pool:\n        for _ in tqdm.tqdm(pool.imap_unordered(resize_file, fnames), total=len(fnames)):\n            pass\n</code></pre></p> <ol> <li> <p>Run the resize script. This will add the resized images to a new RootPainter dataset in the datasets folder. <pre><code>  python resize_cxr.py ChinaSet_AllFiles/CXR_png/ ~/Desktop/root_painter_sync/datasets/cxr\n</code></pre></p> </li> <li> <p>If you haven't already installed the RootPainter client then download the   client installer for your operating system.   dmg (OSX) exe (Windows) and deb (Debian/Ubuntu) files are available from   this link.   Then run the downloaded installer.</p> </li> <li> <p>When you open the installed application for the first time. You will be promped to specify a sync directory. Specify ~/Desktop/root_painter_sync. The specified path will be added to a settings file in your home folder named root_painter_settings.json and the value found in that file will be used from then on.</p> </li> <li> <p>Click the 'Create new project' button which is visible when opening RootPainter.</p> <ul> <li>Specify ~/Desktop/root_painter_sync/datasets/cxr as the image directory.</li> <li>Specify random weights</li> <li>name your project 'cxr_tutorial' (or whatever you want).</li> </ul> </li> <li> <p>Annotate and train.</p> <ul> <li>Label two images. Use the red (foreground) brush for the lungs and the green (background) brush for everything else. The brushes can be selected from the brush menu or using the shortcut key Q for foreground and W for background. Leave ambiguous regions as undefined. It is important to label both some foreground and some background in the first 2 images. Hold alt (on Mac) or shift (on windows) and scroll to change the size of the brush. If you make a mistake you can undo by pressing the Z key or using the eraser tool (key E). Click 'Save &amp; Next' after completing each annotation. Your annotations may look differently depending on your knowledge of lungs, the random order and which images were used for the dataset. </li> </ul> <p></p> <p></p> <ul> <li>Click start training from the network menu.</li> <li>Label a third image in a similar way to the first two.</li> <li> <p>For the fourth image, first view the segmentation to inspect the trained model performance. The segmentation can be shown by ticking the checkbox or pressing the S key. It may be useful to also hide the annotation (key A) and image (key I) when inspecting the segmentation.   </p> </li> <li> <p>Target the annotations towards areas where the segentation is inadequate.   </p> </li> <li> <p>Proceed through the images. First viewing the segmentation and then assigning annotations to correct for mistakes.</p> </li> <li>If waiting for the segmentation is slowing you down then set Pre-Segment from 0 to 1. Pre-Segment is in the options menu.</li> <li> <p>Keep progressing through the images until you are happy with the quality of the model. It's possible to work through over 200 images in two hours but this will depend on your attention to detail and GPU. The segmentation quality will vary but should improve on average, with less annotation being required over time and eventually many of the images will not require corrections allowing you to proceed faster as you continue.</p> <p> </p> </li> </ul> </li> </ol>"}]}